{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "driven-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cbb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "reasonable-davis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TEAM', 'CONF', 'G', 'W', 'ADJOE', 'ADJDE', 'BARTHAG', 'EFG_O', 'EFG_D',\n",
       "       'TOR', 'TORD', 'ORB', 'DRB', 'FTR', 'FTRD', '2P_O', '2P_D', '3P_O',\n",
       "       '3P_D', 'ADJ_T', 'WAB', 'POSTSEASON', 'SEED', 'YEAR'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "grave-result",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM</th>\n",
       "      <th>CONF</th>\n",
       "      <th>G</th>\n",
       "      <th>W</th>\n",
       "      <th>ADJOE</th>\n",
       "      <th>ADJDE</th>\n",
       "      <th>BARTHAG</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>EFG_D</th>\n",
       "      <th>TOR</th>\n",
       "      <th>...</th>\n",
       "      <th>FTRD</th>\n",
       "      <th>2P_O</th>\n",
       "      <th>2P_D</th>\n",
       "      <th>3P_O</th>\n",
       "      <th>3P_D</th>\n",
       "      <th>ADJ_T</th>\n",
       "      <th>WAB</th>\n",
       "      <th>POSTSEASON</th>\n",
       "      <th>SEED</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>ACC</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>123.3</td>\n",
       "      <td>94.9</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>52.6</td>\n",
       "      <td>48.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>...</td>\n",
       "      <td>30.4</td>\n",
       "      <td>53.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>32.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>71.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2ND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>B10</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>129.1</td>\n",
       "      <td>93.6</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>54.8</td>\n",
       "      <td>47.7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>...</td>\n",
       "      <td>22.4</td>\n",
       "      <td>54.8</td>\n",
       "      <td>44.7</td>\n",
       "      <td>36.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>59.3</td>\n",
       "      <td>11.3</td>\n",
       "      <td>2ND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>B10</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>114.4</td>\n",
       "      <td>90.4</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>53.9</td>\n",
       "      <td>47.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54.7</td>\n",
       "      <td>46.8</td>\n",
       "      <td>35.2</td>\n",
       "      <td>33.2</td>\n",
       "      <td>65.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2ND</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Texas Tech</td>\n",
       "      <td>B12</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>115.2</td>\n",
       "      <td>85.2</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>53.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>...</td>\n",
       "      <td>36.6</td>\n",
       "      <td>52.8</td>\n",
       "      <td>41.9</td>\n",
       "      <td>36.5</td>\n",
       "      <td>29.7</td>\n",
       "      <td>67.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2ND</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gonzaga</td>\n",
       "      <td>WCC</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>117.8</td>\n",
       "      <td>86.3</td>\n",
       "      <td>0.9728</td>\n",
       "      <td>56.6</td>\n",
       "      <td>41.1</td>\n",
       "      <td>16.2</td>\n",
       "      <td>...</td>\n",
       "      <td>26.9</td>\n",
       "      <td>56.3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>71.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2ND</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             TEAM CONF   G   W  ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D   TOR  \\\n",
       "0  North Carolina  ACC  40  33  123.3   94.9   0.9531   52.6   48.1  15.4   \n",
       "1       Wisconsin  B10  40  36  129.1   93.6   0.9758   54.8   47.7  12.4   \n",
       "2        Michigan  B10  40  33  114.4   90.4   0.9375   53.9   47.7  14.0   \n",
       "3      Texas Tech  B12  38  31  115.2   85.2   0.9696   53.5   43.0  17.7   \n",
       "4         Gonzaga  WCC  39  37  117.8   86.3   0.9728   56.6   41.1  16.2   \n",
       "\n",
       "   ...  FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T   WAB  POSTSEASON  SEED  YEAR  \n",
       "0  ...  30.4  53.9  44.6  32.7  36.2   71.7   8.6         2ND   1.0  2016  \n",
       "1  ...  22.4  54.8  44.7  36.5  37.5   59.3  11.3         2ND   1.0  2015  \n",
       "2  ...  30.0  54.7  46.8  35.2  33.2   65.9   6.9         2ND   3.0  2018  \n",
       "3  ...  36.6  52.8  41.9  36.5  29.7   67.5   7.0         2ND   3.0  2019  \n",
       "4  ...  26.9  56.3  40.0  38.2  29.0   71.5   7.7         2ND   1.0  2017  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-detection",
   "metadata": {},
   "source": [
    "### Goal: Predict post season of team based on stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "certain-synthetic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R64          224\n",
       "R32          112\n",
       "S16           56\n",
       "R68           28\n",
       "E8            28\n",
       "F4            14\n",
       "2ND            7\n",
       "Champions      7\n",
       "Name: POSTSEASON, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.POSTSEASON.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-district",
   "metadata": {},
   "source": [
    "#### Step 1: Figure out x and y samples\n",
    "<ul>\n",
    "    <li>df.POSTSEASON will be predicted so that is y</li>\n",
    "    <li>Combinations of different of attributes is x </li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "pressing-intellectual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data:\n",
      "       CONF  ADJOE  ADJDE\n",
      "0      ACC  123.3   94.9\n",
      "1      B10  129.1   93.6\n",
      "2      B10  114.4   90.4\n",
      "3      B12  115.2   85.2\n",
      "4      WCC  117.8   86.3\n",
      "...    ...    ...    ...\n",
      "2450   B10  111.4   87.8\n",
      "2451   P12  114.4   92.2\n",
      "2452   P12  104.8   88.6\n",
      "2453   A10  112.0   96.2\n",
      "2454  ASun  103.4   96.3\n",
      "\n",
      "[2455 rows x 3 columns]\n",
      "y data:\n",
      "      POSTSEASON\n",
      "0           2ND\n",
      "1           2ND\n",
      "2           2ND\n",
      "3           2ND\n",
      "4           2ND\n",
      "...         ...\n",
      "2450        S16\n",
      "2451        S16\n",
      "2452        S16\n",
      "2453        S16\n",
      "2454        S16\n",
      "\n",
      "[2455 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def getXY(df, xColumns, yColumns = [\"POSTSEASON\"]):\n",
    "    df = df.copy()\n",
    "    xData = df.loc[:,xColumns]\n",
    "    yData = df.loc[:,yColumns]\n",
    "    return (xData,yData)\n",
    "\n",
    "xData, yData = getXY(df, [\"CONF\", \"ADJOE\", \"ADJDE\"])\n",
    "print(\"x data:\\n\", xData)\n",
    "print(\"y data:\\n\", yData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-cambridge",
   "metadata": {},
   "source": [
    "#### Step 2: Encoding data\n",
    "\n",
    "* df.CONF in xData and df.POSTSEASON in yData are categorical so they must be one hot encoded\n",
    "* Every distinct category in a column gets its own index in a vector of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ordinary-dodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before one hot encoding:\n",
      " 0    ACC\n",
      "1    B10\n",
      "2    B10\n",
      "3    B12\n",
      "4    WCC\n",
      "Name: CONF, dtype: object\n",
      "After one hot encoding:\n",
      " [[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def oneHotEncode(column):\n",
    "    nCategories = len(list(set(column.to_list())))\n",
    "    encoded = []\n",
    "    mapping = {}\n",
    "    currentInd = 0\n",
    "    for value in column:\n",
    "        if(value not in mapping.keys()):\n",
    "            mapping[value] = currentInd\n",
    "            currentInd += 1\n",
    "        encoded.append([0 if i != mapping[value] else 1 for i in range(nCategories)])\n",
    "    return np.array(encoded), mapping\n",
    "\n",
    "\n",
    "CONFohe, CONFmapping = oneHotEncode(xData[\"CONF\"])\n",
    "\n",
    "print(\"Before one hot encoding:\\n\",xData[\"CONF\"][:5])\n",
    "print(\"After one hot encoding:\\n\",CONFohe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-airline",
   "metadata": {},
   "source": [
    "Next the one hot encoded vectors must be combined with non-categorical columns (ADJOE and ADJDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "mexican-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.    0.    0.  ...   0.  123.3  94.9]\n",
      " [  0.    1.    0.  ...   0.  129.1  93.6]\n",
      " [  0.    1.    0.  ...   0.  114.4  90.4]\n",
      " ...\n",
      " [  0.    0.    0.  ...   0.  104.8  88.6]\n",
      " [  0.    0.    0.  ...   0.  112.   96.2]\n",
      " [  0.    0.    0.  ...   0.  103.4  96.3]]\n"
     ]
    }
   ],
   "source": [
    "xEncoded = np.concatenate([CONFohe, xData.iloc[:,[1,2]]], axis = 1)\n",
    "print(xEncoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "approximate-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "yEncoded, POSTSEASONmapping = oneHotEncode(yData[\"POSTSEASON\"])\n",
    "print(yEncoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-shift",
   "metadata": {},
   "source": [
    "#### Step 3: Train test split\n",
    "\n",
    "* Train data used to train\n",
    "* Test data used to evaluate performance on unseen data\n",
    "* Test data usually ~20% of data\n",
    "* Train_test_split in sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "synthetic-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions\n",
      "----------\n",
      "Training data:  (1964, 37) (1964, 9)\n",
      "Testing data:  (491, 37) (491, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xEncoded,yEncoded, test_size = 0.2)\n",
    "\n",
    "print(\"Dimensions\")\n",
    "print(\"----------\")\n",
    "print(\"Training data: \", xTrain.shape, yTrain.shape)\n",
    "print(\"Testing data: \",xTest.shape, yTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-hepatitis",
   "metadata": {},
   "source": [
    "#### Step 4: Model creation\n",
    "* Vanilla neural network \n",
    "* Start simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "structural-shareware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 16)                608       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 9)                 153       \n",
      "=================================================================\n",
      "Total params: 761\n",
      "Trainable params: 761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "inputDim = 37\n",
    "outputDim = 9\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation = 'relu', input_shape = (37,)))\n",
    "model.add(Dense(outputDim, activation = 'softmax'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "better-tradition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "62/62 [==============================] - 1s 7ms/step - loss: 9.9488 - accuracy: 0.3227 - val_loss: 2.2697 - val_accuracy: 0.8126\n",
      "Epoch 2/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 2.0521 - accuracy: 0.8061 - val_loss: 1.2101 - val_accuracy: 0.8187\n",
      "Epoch 3/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.8949 - accuracy: 0.8254 - val_loss: 0.8340 - val_accuracy: 0.8208\n",
      "Epoch 4/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6252 - accuracy: 0.8348 - val_loss: 0.7741 - val_accuracy: 0.8248\n",
      "Epoch 5/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6403 - accuracy: 0.8138 - val_loss: 0.7601 - val_accuracy: 0.8228\n",
      "Epoch 6/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6331 - accuracy: 0.8357 - val_loss: 0.7233 - val_accuracy: 0.8228\n",
      "Epoch 7/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.7585 - accuracy: 0.8187 - val_loss: 0.7092 - val_accuracy: 0.8147\n",
      "Epoch 8/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6192 - accuracy: 0.8347 - val_loss: 0.6926 - val_accuracy: 0.8187\n",
      "Epoch 9/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6063 - accuracy: 0.8165 - val_loss: 0.6713 - val_accuracy: 0.8147\n",
      "Epoch 10/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5590 - accuracy: 0.8407 - val_loss: 0.6416 - val_accuracy: 0.8208\n",
      "Epoch 11/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5494 - accuracy: 0.8276 - val_loss: 0.6230 - val_accuracy: 0.8187\n",
      "Epoch 12/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5205 - accuracy: 0.8250 - val_loss: 0.6166 - val_accuracy: 0.8228\n",
      "Epoch 13/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5014 - accuracy: 0.8301 - val_loss: 0.5785 - val_accuracy: 0.8208\n",
      "Epoch 14/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4940 - accuracy: 0.8203 - val_loss: 0.5879 - val_accuracy: 0.8208\n",
      "Epoch 15/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5031 - accuracy: 0.8363 - val_loss: 0.5755 - val_accuracy: 0.8187\n",
      "Epoch 16/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4798 - accuracy: 0.8285 - val_loss: 0.5762 - val_accuracy: 0.8147\n",
      "Epoch 17/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4677 - accuracy: 0.8368 - val_loss: 0.5507 - val_accuracy: 0.8208\n",
      "Epoch 18/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4897 - accuracy: 0.8263 - val_loss: 0.5521 - val_accuracy: 0.8228\n",
      "Epoch 19/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4810 - accuracy: 0.8309 - val_loss: 0.5546 - val_accuracy: 0.8187\n",
      "Epoch 20/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4892 - accuracy: 0.8292 - val_loss: 0.5642 - val_accuracy: 0.8248\n",
      "Epoch 21/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.8343 - val_loss: 0.5661 - val_accuracy: 0.8269\n",
      "Epoch 22/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5166 - accuracy: 0.8161 - val_loss: 0.5807 - val_accuracy: 0.8187\n",
      "Epoch 23/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4600 - accuracy: 0.8402 - val_loss: 0.5739 - val_accuracy: 0.8187\n",
      "Epoch 24/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4987 - accuracy: 0.8224 - val_loss: 0.5637 - val_accuracy: 0.8269\n",
      "Epoch 25/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4566 - accuracy: 0.8396 - val_loss: 0.5542 - val_accuracy: 0.8187\n",
      "Epoch 26/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4974 - accuracy: 0.8283 - val_loss: 0.5584 - val_accuracy: 0.8208\n",
      "Epoch 27/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4130 - accuracy: 0.8589 - val_loss: 0.5605 - val_accuracy: 0.8248\n",
      "Epoch 28/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5208 - accuracy: 0.8137 - val_loss: 0.5550 - val_accuracy: 0.8228\n",
      "Epoch 29/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4755 - accuracy: 0.8247 - val_loss: 0.5626 - val_accuracy: 0.8228\n",
      "Epoch 30/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4736 - accuracy: 0.8280 - val_loss: 0.5733 - val_accuracy: 0.8208\n",
      "Epoch 31/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4634 - accuracy: 0.8416 - val_loss: 0.5800 - val_accuracy: 0.8228\n",
      "Epoch 32/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4716 - accuracy: 0.8333 - val_loss: 0.5529 - val_accuracy: 0.8167\n",
      "Epoch 33/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4679 - accuracy: 0.8328 - val_loss: 0.5577 - val_accuracy: 0.8208\n",
      "Epoch 34/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4749 - accuracy: 0.8248 - val_loss: 0.5643 - val_accuracy: 0.8248\n",
      "Epoch 35/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4716 - accuracy: 0.8352 - val_loss: 0.5653 - val_accuracy: 0.8147\n",
      "Epoch 36/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4464 - accuracy: 0.8402 - val_loss: 0.5483 - val_accuracy: 0.8208\n",
      "Epoch 37/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4535 - accuracy: 0.8378 - val_loss: 0.5557 - val_accuracy: 0.8289\n",
      "Epoch 38/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4845 - accuracy: 0.8282 - val_loss: 0.5509 - val_accuracy: 0.8167\n",
      "Epoch 39/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.8428 - val_loss: 0.5492 - val_accuracy: 0.8228\n",
      "Epoch 40/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4480 - accuracy: 0.8412 - val_loss: 0.5779 - val_accuracy: 0.8147\n",
      "Epoch 41/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.8331 - val_loss: 0.5737 - val_accuracy: 0.8228\n",
      "Epoch 42/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.8315 - val_loss: 0.5437 - val_accuracy: 0.8208\n",
      "Epoch 43/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4786 - accuracy: 0.8304 - val_loss: 0.5455 - val_accuracy: 0.8248\n",
      "Epoch 44/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4720 - accuracy: 0.8331 - val_loss: 0.5620 - val_accuracy: 0.8248\n",
      "Epoch 45/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4554 - accuracy: 0.8346 - val_loss: 0.5403 - val_accuracy: 0.8269\n",
      "Epoch 46/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.8279 - val_loss: 0.5565 - val_accuracy: 0.8208\n",
      "Epoch 47/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.8304 - val_loss: 0.5449 - val_accuracy: 0.8289\n",
      "Epoch 48/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4610 - accuracy: 0.8350 - val_loss: 0.5592 - val_accuracy: 0.8228\n",
      "Epoch 49/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4670 - accuracy: 0.8370 - val_loss: 0.5296 - val_accuracy: 0.8269\n",
      "Epoch 50/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4714 - accuracy: 0.8362 - val_loss: 0.5325 - val_accuracy: 0.8228\n",
      "Epoch 51/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4681 - accuracy: 0.8345 - val_loss: 0.5459 - val_accuracy: 0.8208\n",
      "Epoch 52/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4366 - accuracy: 0.8509 - val_loss: 0.5648 - val_accuracy: 0.8106\n",
      "Epoch 53/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4900 - accuracy: 0.8212 - val_loss: 0.5532 - val_accuracy: 0.8289\n",
      "Epoch 54/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4492 - accuracy: 0.8390 - val_loss: 0.5574 - val_accuracy: 0.8126\n",
      "Epoch 55/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4496 - accuracy: 0.8412 - val_loss: 0.5490 - val_accuracy: 0.8289\n",
      "Epoch 56/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4369 - accuracy: 0.8393 - val_loss: 0.5305 - val_accuracy: 0.8208\n",
      "Epoch 57/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4799 - accuracy: 0.8333 - val_loss: 0.5529 - val_accuracy: 0.8269\n",
      "Epoch 58/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4513 - accuracy: 0.8386 - val_loss: 0.5437 - val_accuracy: 0.8167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4256 - accuracy: 0.8488 - val_loss: 0.5477 - val_accuracy: 0.8228\n",
      "Epoch 60/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4382 - accuracy: 0.8382 - val_loss: 0.5281 - val_accuracy: 0.8289\n",
      "Epoch 61/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4383 - accuracy: 0.8482 - val_loss: 0.5464 - val_accuracy: 0.8269\n",
      "Epoch 62/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4435 - accuracy: 0.8410 - val_loss: 0.5238 - val_accuracy: 0.8228\n",
      "Epoch 63/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4597 - accuracy: 0.8303 - val_loss: 0.5591 - val_accuracy: 0.8269\n",
      "Epoch 64/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4645 - accuracy: 0.8250 - val_loss: 0.5407 - val_accuracy: 0.8248\n",
      "Epoch 65/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4239 - accuracy: 0.8517 - val_loss: 0.5232 - val_accuracy: 0.8289\n",
      "Epoch 66/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4699 - accuracy: 0.8266 - val_loss: 0.5334 - val_accuracy: 0.8248\n",
      "Epoch 67/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4154 - accuracy: 0.8494 - val_loss: 0.5393 - val_accuracy: 0.8269\n",
      "Epoch 68/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4614 - accuracy: 0.8244 - val_loss: 0.5523 - val_accuracy: 0.8310\n",
      "Epoch 69/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.8320 - val_loss: 0.5491 - val_accuracy: 0.8167\n",
      "Epoch 70/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4374 - accuracy: 0.8520 - val_loss: 0.5237 - val_accuracy: 0.8310\n",
      "Epoch 71/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4794 - accuracy: 0.8200 - val_loss: 0.5314 - val_accuracy: 0.8228\n",
      "Epoch 72/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4000 - accuracy: 0.8602 - val_loss: 0.5293 - val_accuracy: 0.8330\n",
      "Epoch 73/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4391 - accuracy: 0.8252 - val_loss: 0.5422 - val_accuracy: 0.8187\n",
      "Epoch 74/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4330 - accuracy: 0.8482 - val_loss: 0.5441 - val_accuracy: 0.8289\n",
      "Epoch 75/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4684 - accuracy: 0.8288 - val_loss: 0.5343 - val_accuracy: 0.8228\n",
      "Epoch 76/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4014 - accuracy: 0.8595 - val_loss: 0.5257 - val_accuracy: 0.8187\n",
      "Epoch 77/100\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.4291 - accuracy: 0.8459 - val_loss: 0.5501 - val_accuracy: 0.8248\n",
      "Epoch 78/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4632 - accuracy: 0.8260 - val_loss: 0.5472 - val_accuracy: 0.8208\n",
      "Epoch 79/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4147 - accuracy: 0.8461 - val_loss: 0.6159 - val_accuracy: 0.7902\n",
      "Epoch 80/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4675 - accuracy: 0.8271 - val_loss: 0.5359 - val_accuracy: 0.8269\n",
      "Epoch 81/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4416 - accuracy: 0.8380 - val_loss: 0.5433 - val_accuracy: 0.8167\n",
      "Epoch 82/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4417 - accuracy: 0.8485 - val_loss: 0.5462 - val_accuracy: 0.8167\n",
      "Epoch 83/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4450 - accuracy: 0.8404 - val_loss: 0.5473 - val_accuracy: 0.8106\n",
      "Epoch 84/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4154 - accuracy: 0.8520 - val_loss: 0.5229 - val_accuracy: 0.8228\n",
      "Epoch 85/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.8427 - val_loss: 0.5348 - val_accuracy: 0.8289\n",
      "Epoch 86/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4089 - accuracy: 0.8512 - val_loss: 0.5486 - val_accuracy: 0.8086\n",
      "Epoch 87/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4241 - accuracy: 0.8487 - val_loss: 0.5310 - val_accuracy: 0.8228\n",
      "Epoch 88/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4337 - accuracy: 0.8398 - val_loss: 0.5311 - val_accuracy: 0.8187\n",
      "Epoch 89/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4315 - accuracy: 0.8503 - val_loss: 0.5516 - val_accuracy: 0.8208\n",
      "Epoch 90/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.8451 - val_loss: 0.5312 - val_accuracy: 0.8269\n",
      "Epoch 91/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4266 - accuracy: 0.8526 - val_loss: 0.5278 - val_accuracy: 0.8167\n",
      "Epoch 92/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4030 - accuracy: 0.8435 - val_loss: 0.5370 - val_accuracy: 0.8228\n",
      "Epoch 93/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4272 - accuracy: 0.8524 - val_loss: 0.5438 - val_accuracy: 0.8065\n",
      "Epoch 94/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4545 - accuracy: 0.8224 - val_loss: 0.5224 - val_accuracy: 0.8269\n",
      "Epoch 95/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4085 - accuracy: 0.8480 - val_loss: 0.5256 - val_accuracy: 0.8187\n",
      "Epoch 96/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4085 - accuracy: 0.8509 - val_loss: 0.5741 - val_accuracy: 0.8147\n",
      "Epoch 97/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4639 - accuracy: 0.8450 - val_loss: 0.5331 - val_accuracy: 0.8208\n",
      "Epoch 98/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.8326 - val_loss: 0.5261 - val_accuracy: 0.8208\n",
      "Epoch 99/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4137 - accuracy: 0.8514 - val_loss: 0.5245 - val_accuracy: 0.8248\n",
      "Epoch 100/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4216 - accuracy: 0.8436 - val_loss: 0.5259 - val_accuracy: 0.8248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25059af60a0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xTrain,yTrain, validation_data = (xTest,yTest), epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-spanking",
   "metadata": {},
   "source": [
    "~82% of time this model makes the correct prediction. Not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "present-equivalent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1644834e-02 1.9341450e-02 5.5187024e-02 ... 2.0466171e-01\n",
      "  2.6706148e-02 9.5386982e-02]\n",
      " [4.9517467e-03 1.4550710e-02 2.9242275e-02 ... 1.6684827e-01\n",
      "  2.2280121e-02 6.9428876e-02]\n",
      " [5.1646894e-06 1.2868379e-04 4.7356763e-04 ... 1.8912174e-02\n",
      "  4.3660123e-03 3.9262528e-04]\n",
      " ...\n",
      " [1.5193698e-02 1.5270739e-02 4.5832142e-02 ... 2.0135576e-01\n",
      "  2.9491760e-02 6.6213071e-02]\n",
      " [1.0005021e-02 1.3221782e-02 5.1140778e-02 ... 2.8214189e-01\n",
      "  1.7475381e-02 1.5188107e-01]\n",
      " [3.4254765e-05 6.3782372e-04 1.4105245e-03 ... 2.9975075e-02\n",
      "  8.0905156e-03 1.5485827e-03]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(xTest)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "german-logging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "predictedIndices = [np.argmax(sample) for sample in predictions]\n",
    "print(predictedIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "beginning-default",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '2ND',\n",
       " 1: 'Champions',\n",
       " 2: 'E8',\n",
       " 3: 'F4',\n",
       " 4: nan,\n",
       " 5: 'R32',\n",
       " 6: 'R64',\n",
       " 7: 'R68',\n",
       " 8: 'S16'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inversePOSTSEASONmapping = dict([(item[1], item[0]) for item in POSTSEASONmapping.items()])\n",
    "inversePOSTSEASONmapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "occupied-unemployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('R32', 'R32'), (nan, 'S16'), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), ('R32', 'S16'), (nan, nan), (nan, nan), (nan, 'R68'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, 'R64'), (nan, 'R32'), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, nan), ('R32', 'S16'), (nan, 'R32'), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, 'R64'), (nan, 'R64'), (nan, nan), ('R32', 'E8'), (nan, nan), ('R32', 'F4'), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', '2ND'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, 'R32'), (nan, 'R64'), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'F4'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R68'), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R68'), ('R32', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R68'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R68'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, 'R64'), (nan, 'R32'), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), ('R32', 'E8'), (nan, nan), (nan, nan), ('R32', 'S16'), (nan, nan), ('R32', 'R32'), ('R32', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), ('R32', 'E8'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, 'R64'), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), ('R64', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'F4'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'E8'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), ('R64', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R64'), (nan, nan), (nan, 'R68'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, 'R64'), ('R32', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, nan), (nan, 'R64'), (nan, nan), (nan, 'R68'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, 'S16'), (nan, nan), (nan, 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), ('R32', 'R32'), (nan, nan), (nan, nan), ('R32', nan), (nan, nan), (nan, nan), ('R32', 'Champions'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), ('R32', 'F4'), (nan, nan), (nan, nan), (nan, 'S16'), (nan, nan), (nan, nan), (nan, nan), ('R32', 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R32'), ('R32', nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'S16'), (nan, nan), (nan, nan), (nan, 'R64'), (nan, 'S16'), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'E8'), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, 'E8'), (nan, nan), (nan, 'S16'), ('R64', 'R64'), (nan, nan)]\n"
     ]
    }
   ],
   "source": [
    "print([(inversePOSTSEASONmapping[predictedIndices[i]], inversePOSTSEASONmapping[np.argmax(yTest[i])]) for i in range(len(predictedIndices))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-auckland",
   "metadata": {},
   "source": [
    "Often the more information that you allow the network to access, the better it performs. But if the information is redundent there won't be improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "banned-migration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2P_O</th>\n",
       "      <th>2P_D</th>\n",
       "      <th>3P_O</th>\n",
       "      <th>3P_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>32.7</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.8</td>\n",
       "      <td>44.7</td>\n",
       "      <td>36.5</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.7</td>\n",
       "      <td>46.8</td>\n",
       "      <td>35.2</td>\n",
       "      <td>33.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.8</td>\n",
       "      <td>41.9</td>\n",
       "      <td>36.5</td>\n",
       "      <td>29.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.2</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>50.4</td>\n",
       "      <td>44.3</td>\n",
       "      <td>34.1</td>\n",
       "      <td>30.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>50.6</td>\n",
       "      <td>43.4</td>\n",
       "      <td>37.1</td>\n",
       "      <td>35.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>49.1</td>\n",
       "      <td>44.9</td>\n",
       "      <td>33.3</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>49.3</td>\n",
       "      <td>50.6</td>\n",
       "      <td>37.7</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>52.3</td>\n",
       "      <td>46.9</td>\n",
       "      <td>33.4</td>\n",
       "      <td>31.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2455 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2P_O  2P_D  3P_O  3P_D\n",
       "0     53.9  44.6  32.7  36.2\n",
       "1     54.8  44.7  36.5  37.5\n",
       "2     54.7  46.8  35.2  33.2\n",
       "3     52.8  41.9  36.5  29.7\n",
       "4     56.3  40.0  38.2  29.0\n",
       "...    ...   ...   ...   ...\n",
       "2450  50.4  44.3  34.1  30.1\n",
       "2451  50.6  43.4  37.1  35.8\n",
       "2452  49.1  44.9  33.3  33.4\n",
       "2453  49.3  50.6  37.7  30.2\n",
       "2454  52.3  46.9  33.4  31.3\n",
       "\n",
       "[2455 rows x 4 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newData = df.loc[:, [\"2P_O\", \"2P_D\", \"3P_O\", \"3P_D\"]]\n",
    "newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "optional-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2455, 41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0. ,  0. , ..., 44.6, 32.7, 36.2],\n",
       "       [ 0. ,  1. ,  0. , ..., 44.7, 36.5, 37.5],\n",
       "       [ 0. ,  1. ,  0. , ..., 46.8, 35.2, 33.2],\n",
       "       ...,\n",
       "       [ 0. ,  0. ,  0. , ..., 44.9, 33.3, 33.4],\n",
       "       [ 0. ,  0. ,  0. , ..., 50.6, 37.7, 30.2],\n",
       "       [ 0. ,  0. ,  0. , ..., 46.9, 33.4, 31.3]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xEncodedWithNew = np.concatenate([xEncoded, newData], axis = 1)\n",
    "print(xEncodedWithNew.shape)\n",
    "xEncodedWithNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "latest-involvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 16)                608       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 9)                 153       \n",
      "=================================================================\n",
      "Total params: 761\n",
      "Trainable params: 761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(xEncoded,yEncoded, test_size = 0.2)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "inputDim = 41\n",
    "outputDim = 9\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation = 'relu', input_shape = (37,)))\n",
    "model.add(Dense(outputDim, activation = 'softmax'))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "western-republic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "62/62 [==============================] - 1s 6ms/step - loss: 7.8455 - accuracy: 0.7999 - val_loss: 3.1692 - val_accuracy: 0.7617\n",
      "Epoch 2/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 3.2373 - accuracy: 0.7099 - val_loss: 2.3776 - val_accuracy: 0.7454\n",
      "Epoch 3/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 2.4395 - accuracy: 0.7406 - val_loss: 1.8245 - val_accuracy: 0.6640\n",
      "Epoch 4/100\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.1000 - accuracy: 0.7528 - val_loss: 1.2596 - val_accuracy: 0.8187\n",
      "Epoch 5/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2769 - accuracy: 0.7961 - val_loss: 0.8649 - val_accuracy: 0.8187\n",
      "Epoch 6/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.9073 - accuracy: 0.7884 - val_loss: 0.7391 - val_accuracy: 0.8187\n",
      "Epoch 7/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.7153 - accuracy: 0.8010 - val_loss: 0.6626 - val_accuracy: 0.8187\n",
      "Epoch 8/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6474 - accuracy: 0.8093 - val_loss: 0.6360 - val_accuracy: 0.8208\n",
      "Epoch 9/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6191 - accuracy: 0.8186 - val_loss: 0.6096 - val_accuracy: 0.8208\n",
      "Epoch 10/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.6029 - accuracy: 0.8097 - val_loss: 0.5857 - val_accuracy: 0.8187\n",
      "Epoch 11/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5970 - accuracy: 0.8102 - val_loss: 0.5846 - val_accuracy: 0.8208\n",
      "Epoch 12/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5390 - accuracy: 0.8323 - val_loss: 0.5661 - val_accuracy: 0.8228\n",
      "Epoch 13/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5489 - accuracy: 0.8308 - val_loss: 0.5893 - val_accuracy: 0.8289\n",
      "Epoch 14/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5828 - accuracy: 0.8117 - val_loss: 0.5707 - val_accuracy: 0.8208\n",
      "Epoch 15/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5793 - accuracy: 0.8029 - val_loss: 0.5491 - val_accuracy: 0.8228\n",
      "Epoch 16/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5766 - accuracy: 0.8122 - val_loss: 0.5505 - val_accuracy: 0.8350\n",
      "Epoch 17/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5812 - accuracy: 0.8182 - val_loss: 0.5372 - val_accuracy: 0.8228\n",
      "Epoch 18/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5613 - accuracy: 0.8208 - val_loss: 0.5586 - val_accuracy: 0.8350\n",
      "Epoch 19/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5575 - accuracy: 0.8202 - val_loss: 0.5310 - val_accuracy: 0.8248\n",
      "Epoch 20/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5248 - accuracy: 0.8330 - val_loss: 0.5340 - val_accuracy: 0.8269\n",
      "Epoch 21/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5306 - accuracy: 0.8199 - val_loss: 0.5280 - val_accuracy: 0.8310\n",
      "Epoch 22/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5074 - accuracy: 0.8323 - val_loss: 0.5262 - val_accuracy: 0.8289\n",
      "Epoch 23/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5193 - accuracy: 0.8347 - val_loss: 0.5189 - val_accuracy: 0.8310\n",
      "Epoch 24/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5243 - accuracy: 0.8234 - val_loss: 0.5106 - val_accuracy: 0.8310\n",
      "Epoch 25/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5251 - accuracy: 0.8232 - val_loss: 0.5191 - val_accuracy: 0.8330\n",
      "Epoch 26/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5441 - accuracy: 0.8207 - val_loss: 0.5296 - val_accuracy: 0.8187\n",
      "Epoch 27/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5505 - accuracy: 0.8129 - val_loss: 0.5202 - val_accuracy: 0.8310\n",
      "Epoch 28/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5391 - accuracy: 0.8104 - val_loss: 0.5224 - val_accuracy: 0.8289\n",
      "Epoch 29/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5588 - accuracy: 0.8107 - val_loss: 0.5084 - val_accuracy: 0.8350\n",
      "Epoch 30/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5215 - accuracy: 0.8320 - val_loss: 0.4985 - val_accuracy: 0.8310\n",
      "Epoch 31/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.8180 - val_loss: 0.5103 - val_accuracy: 0.8350\n",
      "Epoch 32/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5447 - accuracy: 0.8281 - val_loss: 0.5214 - val_accuracy: 0.8350\n",
      "Epoch 33/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5296 - accuracy: 0.8238 - val_loss: 0.4955 - val_accuracy: 0.8269\n",
      "Epoch 34/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5138 - accuracy: 0.8284 - val_loss: 0.4999 - val_accuracy: 0.8310\n",
      "Epoch 35/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5014 - accuracy: 0.8344 - val_loss: 0.4997 - val_accuracy: 0.8350\n",
      "Epoch 36/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4790 - accuracy: 0.8448 - val_loss: 0.4938 - val_accuracy: 0.8371\n",
      "Epoch 37/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5070 - accuracy: 0.8320 - val_loss: 0.5051 - val_accuracy: 0.8269\n",
      "Epoch 38/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5182 - accuracy: 0.8235 - val_loss: 0.5082 - val_accuracy: 0.8269\n",
      "Epoch 39/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5028 - accuracy: 0.8277 - val_loss: 0.4941 - val_accuracy: 0.8350\n",
      "Epoch 40/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4688 - accuracy: 0.8359 - val_loss: 0.4880 - val_accuracy: 0.8310\n",
      "Epoch 41/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5018 - accuracy: 0.8317 - val_loss: 0.4952 - val_accuracy: 0.8350\n",
      "Epoch 42/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5168 - accuracy: 0.8098 - val_loss: 0.4972 - val_accuracy: 0.8289\n",
      "Epoch 43/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5155 - accuracy: 0.8269 - val_loss: 0.4854 - val_accuracy: 0.8330\n",
      "Epoch 44/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4846 - accuracy: 0.8353 - val_loss: 0.5251 - val_accuracy: 0.8310\n",
      "Epoch 45/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4843 - accuracy: 0.8334 - val_loss: 0.4906 - val_accuracy: 0.8330\n",
      "Epoch 46/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4880 - accuracy: 0.8344 - val_loss: 0.4774 - val_accuracy: 0.8391\n",
      "Epoch 47/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4752 - accuracy: 0.8356 - val_loss: 0.4966 - val_accuracy: 0.8310\n",
      "Epoch 48/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4733 - accuracy: 0.8351 - val_loss: 0.4942 - val_accuracy: 0.8248\n",
      "Epoch 49/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4807 - accuracy: 0.8404 - val_loss: 0.4848 - val_accuracy: 0.8289\n",
      "Epoch 50/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4835 - accuracy: 0.8239 - val_loss: 0.5352 - val_accuracy: 0.8350\n",
      "Epoch 51/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5057 - accuracy: 0.8348 - val_loss: 0.4842 - val_accuracy: 0.8310\n",
      "Epoch 52/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.8527 - val_loss: 0.4940 - val_accuracy: 0.8330\n",
      "Epoch 53/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5129 - accuracy: 0.8277 - val_loss: 0.4857 - val_accuracy: 0.8310\n",
      "Epoch 54/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5229 - accuracy: 0.8130 - val_loss: 0.4850 - val_accuracy: 0.8330\n",
      "Epoch 55/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4696 - accuracy: 0.8398 - val_loss: 0.4745 - val_accuracy: 0.8411\n",
      "Epoch 56/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4892 - accuracy: 0.8312 - val_loss: 0.4761 - val_accuracy: 0.8371\n",
      "Epoch 57/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4906 - accuracy: 0.8316 - val_loss: 0.4725 - val_accuracy: 0.8391\n",
      "Epoch 58/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4854 - accuracy: 0.8391 - val_loss: 0.4726 - val_accuracy: 0.8310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4940 - accuracy: 0.8394 - val_loss: 0.4910 - val_accuracy: 0.8371\n",
      "Epoch 60/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4745 - accuracy: 0.8313 - val_loss: 0.4752 - val_accuracy: 0.8330\n",
      "Epoch 61/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4979 - accuracy: 0.8210 - val_loss: 0.4969 - val_accuracy: 0.8269\n",
      "Epoch 62/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4744 - accuracy: 0.8314 - val_loss: 0.4810 - val_accuracy: 0.8391\n",
      "Epoch 63/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4574 - accuracy: 0.8426 - val_loss: 0.5120 - val_accuracy: 0.8310\n",
      "Epoch 64/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4866 - accuracy: 0.8338 - val_loss: 0.4983 - val_accuracy: 0.8228\n",
      "Epoch 65/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4805 - accuracy: 0.8419 - val_loss: 0.4688 - val_accuracy: 0.8371\n",
      "Epoch 66/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4772 - accuracy: 0.8272 - val_loss: 0.4760 - val_accuracy: 0.8269\n",
      "Epoch 67/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4557 - accuracy: 0.8487 - val_loss: 0.4780 - val_accuracy: 0.8350\n",
      "Epoch 68/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4794 - accuracy: 0.8217 - val_loss: 0.4689 - val_accuracy: 0.8330\n",
      "Epoch 69/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4729 - accuracy: 0.8383 - val_loss: 0.4649 - val_accuracy: 0.8289\n",
      "Epoch 70/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4708 - accuracy: 0.8353 - val_loss: 0.4696 - val_accuracy: 0.8310\n",
      "Epoch 71/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4712 - accuracy: 0.8344 - val_loss: 0.4640 - val_accuracy: 0.8371\n",
      "Epoch 72/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4825 - accuracy: 0.8222 - val_loss: 0.4697 - val_accuracy: 0.8350\n",
      "Epoch 73/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4494 - accuracy: 0.8398 - val_loss: 0.4659 - val_accuracy: 0.8371\n",
      "Epoch 74/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4605 - accuracy: 0.8395 - val_loss: 0.4715 - val_accuracy: 0.8371\n",
      "Epoch 75/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4815 - accuracy: 0.8221 - val_loss: 0.4692 - val_accuracy: 0.8371\n",
      "Epoch 76/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4455 - accuracy: 0.8401 - val_loss: 0.5266 - val_accuracy: 0.8187\n",
      "Epoch 77/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5042 - accuracy: 0.8272 - val_loss: 0.4658 - val_accuracy: 0.8310\n",
      "Epoch 78/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4901 - accuracy: 0.8276 - val_loss: 0.4583 - val_accuracy: 0.8391\n",
      "Epoch 79/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4594 - accuracy: 0.8411 - val_loss: 0.4667 - val_accuracy: 0.8411\n",
      "Epoch 80/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4841 - accuracy: 0.8351 - val_loss: 0.4775 - val_accuracy: 0.8330\n",
      "Epoch 81/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4596 - accuracy: 0.8262 - val_loss: 0.4620 - val_accuracy: 0.8310\n",
      "Epoch 82/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4706 - accuracy: 0.8421 - val_loss: 0.4574 - val_accuracy: 0.8391\n",
      "Epoch 83/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4816 - accuracy: 0.8394 - val_loss: 0.4674 - val_accuracy: 0.8371\n",
      "Epoch 84/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4731 - accuracy: 0.8263 - val_loss: 0.4691 - val_accuracy: 0.8371\n",
      "Epoch 85/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4750 - accuracy: 0.8322 - val_loss: 0.4636 - val_accuracy: 0.8289\n",
      "Epoch 86/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4743 - accuracy: 0.8316 - val_loss: 0.4594 - val_accuracy: 0.8391\n",
      "Epoch 87/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.5094 - accuracy: 0.8174 - val_loss: 0.4592 - val_accuracy: 0.8411\n",
      "Epoch 88/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.8384 - val_loss: 0.4644 - val_accuracy: 0.8350\n",
      "Epoch 89/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4783 - accuracy: 0.8287 - val_loss: 0.4538 - val_accuracy: 0.8350\n",
      "Epoch 90/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4461 - accuracy: 0.8501 - val_loss: 0.4607 - val_accuracy: 0.8391\n",
      "Epoch 91/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4508 - accuracy: 0.8448 - val_loss: 0.4526 - val_accuracy: 0.8432\n",
      "Epoch 92/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.8469 - val_loss: 0.4549 - val_accuracy: 0.8310\n",
      "Epoch 93/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4697 - accuracy: 0.8306 - val_loss: 0.4596 - val_accuracy: 0.8330\n",
      "Epoch 94/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4486 - accuracy: 0.8431 - val_loss: 0.4556 - val_accuracy: 0.8391\n",
      "Epoch 95/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4851 - accuracy: 0.8316 - val_loss: 0.4619 - val_accuracy: 0.8391\n",
      "Epoch 96/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.8326 - val_loss: 0.4865 - val_accuracy: 0.8350\n",
      "Epoch 97/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4598 - accuracy: 0.8394 - val_loss: 0.4704 - val_accuracy: 0.8350\n",
      "Epoch 98/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4502 - accuracy: 0.8387 - val_loss: 0.4626 - val_accuracy: 0.8391\n",
      "Epoch 99/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4394 - accuracy: 0.8498 - val_loss: 0.4578 - val_accuracy: 0.8350\n",
      "Epoch 100/100\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 0.4801 - accuracy: 0.8334 - val_loss: 0.4518 - val_accuracy: 0.8411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25054497730>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xTrain,yTrain, validation_data = (xTest,yTest), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "applied-velvet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(nan, nan), (nan, nan), (nan, nan), (nan, nan), ('S16', 'R32'), ('R64', nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R64', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('S16', 'F4'), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R64', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), ('R64', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R64', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), ('R64', nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R64', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('E8', '2ND'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', nan), ('R64', 'R64'), (nan, nan), (nan, nan), (nan, nan), ('R32', 'F4'), (nan, nan), (nan, 'R64'), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', '2ND'), (nan, nan), ('R64', 'R64'), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), ('R64', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('S16', 'E8'), (nan, nan), ('R64', 'S16'), ('R64', 'R68'), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, nan), ('R64', 'R64'), ('R64', 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, 'R68'), (nan, nan), ('R32', 'R32'), (nan, nan), ('R64', 'R64'), (nan, nan), ('R64', 'E8'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('S16', 'E8'), (nan, nan), (nan, 'R64'), (nan, nan), ('R32', 'S16'), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'F4'), (nan, nan), ('R32', 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R64', 'R64'), (nan, 'R64'), ('R64', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, 'R32'), (nan, nan), (nan, nan), (nan, 'R68'), (nan, nan), (nan, nan), ('S16', nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), ('R64', 'R32'), (nan, nan), (nan, nan), (nan, nan), ('R64', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, 'R64'), ('R64', nan), (nan, nan), (nan, 'R64'), (nan, nan), ('S16', 'R32'), (nan, nan), (nan, 'R64'), ('R64', 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R64', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, 'R64'), ('R32', 'S16'), (nan, nan), (nan, nan), (nan, nan), ('R64', 'S16'), (nan, nan), (nan, 'R68'), (nan, nan), (nan, nan), (nan, nan), ('R32', 'E8'), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('E8', 'E8'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('E8', 'E8'), (nan, nan), (nan, nan), ('R32', 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), ('R32', 'R32'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R68'), (nan, 'R68'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('S16', 'S16'), (nan, nan), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R32', 'R64'), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), ('R64', nan), (nan, nan)]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(xTest)\n",
    "predictedIndices = [np.argmax(sample) for sample in predictions]\n",
    "print([(inversePOSTSEASONmapping[predictedIndices[i]], inversePOSTSEASONmapping[np.argmax(yTest[i])]) for i in range(len(predictedIndices))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
